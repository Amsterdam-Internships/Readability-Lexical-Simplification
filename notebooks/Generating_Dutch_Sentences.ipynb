{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Dutch Sentences",
      "provenance": [],
      "mount_file_id": "1OzU2gdLMufkUkohTdZbcZg90omHN-GLR",
      "authorship_tag": "ABX9TyMh/hR+G/TPgZ14TbV/1x4r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amsterdam-Internships/Readability-Lexical-Simplification/blob/master/Generating_Dutch_Sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.2.0\n",
        "!pip install python-docx "
      ],
      "metadata": {
        "id": "e3zGFUPvJTY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "import glob\n",
        "import spacy \n",
        "import nltk\n",
        "\n",
        "import os\n",
        "from docx import Document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kM-hZPNg7SZ",
        "outputId": "d9b75d2b-cade-4189-b278-0955f44e6a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=b4724df34c1440c0305a072e34fbfffda1970e6c1b53bd77dc351d123c26621e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL3WyvAwsQvp",
        "outputId": "e444267a-2cbb-4808-cc74-8fda19ab5c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download nl_core_news_sm"
      ],
      "metadata": {
        "id": "q-dj7i9iICg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"nl_core_news_sm\")"
      ],
      "metadata": {
        "id": "X3c5GR_4I8OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P4C4FzHesEK",
        "outputId": "3e0f5ff2-d1ed-4294-81a5-357a0975e2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "488\n"
          ]
        }
      ],
      "source": [
        "with open (\"/content/moeilijke woorden.txt\",\"r\") as infile:\n",
        "  data = infile.readlines()\n",
        "\n",
        "complex_words = []\n",
        "\n",
        "for line in data:\n",
        "  if line.startswith(\"[\"):\n",
        "    continue\n",
        "  if line.count(\" \")>0:\n",
        "    continue\n",
        "  line = line.strip()\n",
        "  if len(line) <2:\n",
        "    continue\n",
        "  complex_words.append(line)\n",
        "\n",
        "print(len(complex_words ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = []\n",
        "\n",
        "path = '/content/drive/MyDrive/Thesis/notebooks/Preprocessing/Data for Iva'\n",
        "for filename in glob.glob(os.path.join(path, '*.docx')):\n",
        "  #  with open(os.path.join(os.getcwd(), filename), 'r') as f: # open in readonly mode\n",
        "  doc = Document(filename)\n",
        "  for para in doc.paragraphs:\n",
        "    text = para.text.strip().replace(\"\\n\",\" \")\n",
        "    all_text.append(text)"
      ],
      "metadata": {
        "id": "R69QY4H5gAz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--PpQ7Phqcry",
        "outputId": "0bb326d2-d2c6-4022-a18e-3fbff656d8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/67500193/cleaning-lemmatizing-dutch-dataset-using-spacy\n",
        "def lemmatizer(texts):\n",
        "    texts = [text.replace(\"\\n\", \"\").strip() for text in texts]\n",
        "    docs = nlp.pipe(texts)\n",
        "    cleaned_lemmas = [[t.lemma_ for t in doc] for doc in docs]\n",
        "\n",
        "    return cleaned_lemmas"
      ],
      "metadata": {
        "id": "hX7GS1IGrm3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer(texts):\n",
        "    texts = [text.replace(\"\\n\", \"\").strip() for text in texts]\n",
        "    docs = nlp.pipe(texts)\n",
        "    cleaned_lemmas = [[t.lemma_ for t in doc] for doc in docs]\n",
        "\n",
        "    return cleaned_lemmas\n",
        "text = ['duurzame']\n",
        "lemmatizer(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSTg2BYNygjF",
        "outputId": "af5bbd6d-44e5-41c2-bb03-108cc6622635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['duurzaam']]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complex_sents = []\n",
        "corresponding_cwords = []\n",
        "\n",
        "for paragraph in all_text:\n",
        "\n",
        "  sentences = nltk.sent_tokenize(paragraph)\n",
        "  paragraph_lemmatized = lemmatizer(sentences)\n",
        "\n",
        "  for sent, lem_sent in zip(sentences, paragraph_lemmatized):\n",
        "    sent = sent.replace(\"\\t\",\"\")\n",
        "\n",
        "    for complex_lemma in complex_lemmas:\n",
        "      if len(complex_lemma): \n",
        "        if complex_lemma[0] in lem_sent:\n",
        "          complex_sents.append(sent)\n",
        "          corresponding_cwords.append(complex_lemma)"
      ],
      "metadata": {
        "id": "Vx0hQ2fOsBeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"/content/dutch_sentences.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
        "  for sent, word in zip(complex_sents, corresponding_cwords):\n",
        "    outfile.write(f\"{sent}\\t{word[0]}\\tannotations\\n\")"
      ],
      "metadata": {
        "id": "Bd4OxWYpvYkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}